{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7fmfb0egGfg"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3350,
     "status": "ok",
     "timestamp": 1747947601080,
     "user": {
      "displayName": "Ruiyang Jiang",
      "userId": "00210469439208861786"
     },
     "user_tz": -120
    },
    "id": "UT0R_AGKgVF5",
    "outputId": "9182217e-5aba-4796-f915-3ab57f45b0c8"
   },
   "outputs": [],
   "source": [
    "# NLTK & spaCy 初始化\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "ENTITY_POOL = {\n",
    "    \"PERSON\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Emma\"],\n",
    "    \"ORGANIZATION\": [\"OpenAI\", \"Meta\", \"Stanford\", \"NASA\"],\n",
    "    \"GPE\": [\"Germany\", \"Japan\", \"Kenya\", \"Brazil\", \"India\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 519,
     "status": "ok",
     "timestamp": 1747947602581,
     "user": {
      "displayName": "Ruiyang Jiang",
      "userId": "00210469439208861786"
     },
     "user_tz": -120
    },
    "id": "c8bUd-b9hDLU",
    "outputId": "ea4f5ec9-318d-4df6-cac8-5eddf0018735"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 538,
     "status": "ok",
     "timestamp": 1747947604437,
     "user": {
      "displayName": "Ruiyang Jiang",
      "userId": "00210469439208861786"
     },
     "user_tz": -120
    },
    "id": "YmYlbOQ3hMlK",
    "outputId": "b58e514e-f548-42e1-dba5-a2a218dc2cfb"
   },
   "outputs": [],
   "source": [
    "nltk.download('maxent_ne_chunker_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFA6Vk9AgbHi"
   },
   "outputs": [],
   "source": [
    "def get_antonym(word):\n",
    "    for syn in wordnet.synsets(word, pos=wordnet.ADJ):\n",
    "        for lemma in syn.lemmas():\n",
    "            if lemma.antonyms():\n",
    "                return lemma.antonyms()[0].name()\n",
    "    return None\n",
    "\n",
    "def antonym_replacement(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    tagged = pos_tag(words)\n",
    "    return \" \".join([get_antonym(w.lower()) if tag.startswith(\"JJ\") and get_antonym(w.lower()) else w for w, tag in tagged])\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "def toggle_negation_de_spacy(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    if \"nicht\" in tokens:\n",
    "        return \" \".join([t for t in tokens if t.lower() != \"nicht\"])\n",
    "\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.pos_ == \"VERB\":\n",
    "            return \" \".join(tokens[:i+1] + [\"nicht\"] + tokens[i+1:])\n",
    "\n",
    "    return \" \".join(tokens + [\"nicht\"])\n",
    "\n",
    "def strengthen_modality_de(sentence):\n",
    "    mapping = {\n",
    "        \"kann\": \"muss\",\n",
    "        \"könnte\": \"muss\",\n",
    "        \"dürfte\": \"wird\",\n",
    "        \"sollte\": \"wird\",\n",
    "        \"mag\": \"wird\"\n",
    "    }\n",
    "\n",
    "    words = word_tokenize(sentence, language=\"german\")\n",
    "    new_words = [mapping.get(w.lower(), w) for w in words]\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "\n",
    "def entity_replacement(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    tagged = pos_tag(words)\n",
    "    chunks = ne_chunk(tagged)\n",
    "    new_words = []\n",
    "    for chunk in chunks:\n",
    "        if isinstance(chunk, Tree):\n",
    "            label = chunk.label()\n",
    "            if label in ENTITY_POOL:\n",
    "                new_words.append(random.choice(ENTITY_POOL[label]))\n",
    "            else:\n",
    "                new_words.extend([leaf[0] for leaf in chunk])\n",
    "        else:\n",
    "            new_words.append(chunk[0])\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "def number_replacement(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return \" \".join([str(random.randint(1, 100)) if token.like_num else token.text for token in doc])\n",
    "\n",
    "def generate_variants(sentence, max_variants=3):\n",
    "    funcs = [antonym_replacement, toggle_negation, strengthen_modality, entity_replacement, number_replacement]\n",
    "    variants = set()\n",
    "    queue = [sentence]\n",
    "    while queue and len(variants) < max_variants:\n",
    "        current = queue.pop(0)\n",
    "        for func in funcs:\n",
    "            changed = func(current)\n",
    "            if changed != current and changed not in variants:\n",
    "                variants.add(changed)\n",
    "                queue.append(changed)\n",
    "    return list(variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 533458,
     "status": "ok",
     "timestamp": 1747950373298,
     "user": {
      "displayName": "Ruiyang Jiang",
      "userId": "00210469439208861786"
     },
     "user_tz": -120
    },
    "id": "FLH1cyNZgfzR",
    "outputId": "97427301-8950-4c04-e790-d86aa410acd5"
   },
   "outputs": [],
   "source": [
    "def process_german_augmentation_with_reversed_labels(\n",
    "    input_file,\n",
    "    augmented_output_file,\n",
    "    combined_output_file,\n",
    "    sample_size=800,\n",
    "    variants_per_sentence=2\n",
    "):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as fin:\n",
    "        sentence_pairs = [line.strip().split(\"\\t\", 1) for line in fin if \"\\t\" in line]\n",
    "\n",
    "    print(f\"Orignial: {len(sentence_pairs)}\")\n",
    "\n",
    "    sampled = random.sample(sentence_pairs, sample_size)\n",
    "\n",
    "    augmented_pairs = []\n",
    "\n",
    "    for src, tgt in sampled:\n",
    "        variants = generate_variants(tgt, max_variants=variants_per_sentence)\n",
    "        for v in variants:\n",
    "            # 注意：增强数据标注为 label = 0\n",
    "            augmented_pairs.append((src, v, 0))\n",
    "\n",
    "    # 原始数据标注为 label = 1\n",
    "    original_labeled = [(src, tgt, 1) for src, tgt in sentence_pairs]\n",
    "\n",
    "    # 写增强句对文件\n",
    "    with open(augmented_output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for src, tgt, label in augmented_pairs:\n",
    "            fout.write(f\"{src}\\t{tgt}\\t{label}\\n\")\n",
    "\n",
    "    print(len(augmented_pairs))\n",
    "\n",
    "    # 写原始+增强混合文件\n",
    "    with open(combined_output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for src, tgt, label in original_labeled + augmented_pairs:\n",
    "            fout.write(f\"{src}\\t{tgt}\\t{label}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_german_augmentation_with_reversed_labels(\n",
    "        input_file=\"HSB-DE_train_sampled_10k.tsv\",\n",
    "        augmented_output_file=\"augmented_only.tsv\",\n",
    "        combined_output_file=\"combined_labeled.tsv\",\n",
    "        sample_size=800,\n",
    "        variants_per_sentence=3\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNW17OUk1TGddEpC/iuoLJ5",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
